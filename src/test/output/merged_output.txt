package com.treevalue.quick

import kotlinx.coroutines.*
import java.util.concurrent.ConcurrentHashMap
import java.util.concurrent.atomic.AtomicReference
import java.util.concurrent.locks.ReentrantLock
import kotlin.math.abs
import kotlin.math.min
import kotlin.math.truncate
import kotlin.random.Random

/**
 * Represents a growing medium for neurons, managing their positions and expansion.
 * The layer is a cube centered at (0,0,0).
 *
 * @property initialHalfSideLength The initial half-side length of the cubic layer boundary.
 * @property maxHalfSideLength The maximum half-side length the layer can expand to. Use Float.POSITIVE_INFINITY for unlimited spatial growth.
 * @property maxNeuronCount The maximum number of neurons the layer can hold. Use Long.MAX_VALUE for effectively unlimited neurons.
 * @property growthThreshold The occupancy threshold (fraction of maxNeuronCount) that triggers an expansion attempt. Only applies if maxNeuronCount is finite. Defaults to 0.85f.
 * @property defaultGrowthFactor The default factor by which the layer tries to expand its half-side length (e.g., 1.5 means 1.5x). Defaults to 1.5f.
 * @property eventHandler The EventHandler instance used by newly created neurons.
 */
class Layer(
    initialHalfSideLength: Float = 10.0f,
    val maxHalfSideLength: Float = Float.POSITIVE_INFINITY,
    val maxNeuronCount: Long = Long.MAX_VALUE,
    val growthThreshold: Float = 0.85f,
    val defaultGrowthFactor: Float = 1.5f,
    private val eventHandler: EventHandler
) {

    init {
        require(initialHalfSideLength > 0) { "Initial half-side length must be positive." }
        require(maxHalfSideLength >= initialHalfSideLength) { "Max half-side length must be >= initial half-side length." }
        require(maxNeuronCount > 0) { "Max neuron count must be positive." }
        require(growthThreshold in 0.0f..1.0f) { "Growth threshold must be between 0.0 and 1.0." }
        require(defaultGrowthFactor > 1.0f) { "Default growth factor must be greater than 1.0." }
    }


    private val neurons: ConcurrentHashMap<Position, Neuron> = ConcurrentHashMap()


    private val currentHalfSideLength: AtomicReference<Float> = AtomicReference(initialHalfSideLength)


    private val expansionLock = ReentrantLock()


    private val growthFactors = listOf(defaultGrowthFactor, 1.4f, 1.3f)

    /**
     * @param position The position to check.
     * @return True if a neuron exists at the position and the position is within bounds, false otherwise.
     */
    fun has(position: Position): Boolean {
        val currentBounds = currentHalfSideLength.get()
        return isWithinBounds(position, currentBounds) && neurons.containsKey(position)
    }

    /**
     * @param position The position to query.
     * @return The Neuron at the position, or null if none exists or the position is out of bounds.
     */
    fun getNeuron(position: Position): Neuron? {
        val currentBounds = currentHalfSideLength.get()
        return if (isWithinBounds(position, currentBounds)) {
            neurons[position]
        } else {
            null
        }
    }

    /**
     * Attempts to grow a *new* neuron at the specified position, initiated by a source neuron.
     * Growth only occurs if:
     * 1. The position is within the *current* layer bounds.
     * 2. No neuron already exists at that position.
     * 3. The layer has not reached its maximum neuron count.
     *
     * After successful growth, it checks if the layer needs expansion.
     * This method is thread-safe.
     *
     * @param position The target position for the new neuron.
     * @param sourceNeuron The neuron initiating the growth (used to establish connections).
     * @return The newly created Neuron if growth was successful, null otherwise.
     */
    fun grow(position: Position): Neuron? {
        val currentBounds = currentHalfSideLength.get()
        if (!isWithinBounds(position, currentBounds)) {
            return null
        }

        if (neurons.size >= maxNeuronCount) {
            println("Layer reached max neuron count ($maxNeuronCount). Cannot grow at $position.")
            return null
        }

        val newNeuron = Neuron(coordinate = position, eventHandler = eventHandler)

        val existingNeuron = neurons.putIfAbsent(position, newNeuron)

        if (existingNeuron == null) {
            if (needsExpansion()) {
                tryExpand()
            }
            return newNeuron
        } else {
            println("Growth failed at $position: position already occupied.")
            return null
        }
    }

    /**
     * Gets the current number of neurons in the layer.
     */
    fun getNeuronCount(): Int {
        return neurons.size
    }

    /**
     * Gets the current half-side length of the layer's boundary.
     */
    fun getCurrentHalfSideLength(): Float {
        return currentHalfSideLength.get()
    }

    /**
     * Gets an immutable snapshot of all neurons currently in the layer.
     * Be aware this might be slightly outdated in highly concurrent scenarios.
     */
    fun getAllNeurons(): Collection<Neuron> {
        return neurons.values.toList()
    }


    /**
     * Checks if a position is within the cubic bounds defined by the half-side length.
     */
    private fun isWithinBounds(position: Position, halfSide: Float): Boolean {
        return abs(position.x) <= halfSide &&
                abs(position.y) <= halfSide &&
                abs(position.z) <= halfSide
    }

    private fun needsExpansion(): Boolean {
        if (maxNeuronCount == Long.MAX_VALUE) {
            return false
        }
        val currentSize = neurons.size
        val thresholdCount = (growthThreshold * maxNeuronCount).toLong()
        val currentBounds = currentHalfSideLength.get()
        val canGrowSpatially = currentBounds < maxHalfSideLength
        return currentSize >= thresholdCount && canGrowSpatially
    }

    /**
     * @return True if expansion occurred, false otherwise.
     */
    private fun tryExpand(): Boolean {
        if (!expansionLock.tryLock()) {
            println("Expansion attempt skipped: another thread is already expanding.")
            return false
        }
        try {
            if (!needsExpansion()) {
                println("Expansion check inside lock: Expansion no longer needed.")
                return false
            }
            val currentBounds = currentHalfSideLength.get()
            var expansionSuccessful = false
            for (factor in growthFactors) {
                val potentialNewHalfSide = currentBounds * factor
                val targetHalfSide = min(potentialNewHalfSide, maxHalfSideLength)
                if (targetHalfSide > currentBounds) {
                    val updated = currentHalfSideLength.compareAndSet(currentBounds, targetHalfSide)
                    if (updated) {
                        println("Layer expanded from $currentBounds to $targetHalfSide (Factor: $factor).")
                        expansionSuccessful = true
                        break
                    } else {
                        System.err.println("WARN: Layer expansion CAS failed. Current bounds changed concurrently. Retrying might be needed or another thread succeeded.")
                        expansionSuccessful = false
                        break;
                    }
                } else {
                    println("Expansion attempt with factor $factor stopped: already at or beyond max half-side length ($maxHalfSideLength).")
                }
            }
            if (!expansionSuccessful && currentBounds < maxHalfSideLength) {
                println("Layer expansion failed: None of the growth factors ($growthFactors) resulted in valid expansion below max size ($maxHalfSideLength). Current bounds: $currentBounds")
            } else if (!expansionSuccessful && currentBounds >= maxHalfSideLength) {
                println("Layer expansion not possible: Already at maximum spatial bounds ($maxHalfSideLength).")
            }
            return expansionSuccessful
        } finally {
            expansionLock.unlock()
        }
    }

    override fun toString(): String {
        return "Layer(currentHalfSide=${currentHalfSideLength.get()}, maxHalfSide=$maxHalfSideLength, neuronCount=${neurons.size}, maxNeurons=$maxNeuronCount)"
    }
}

fun main() = runBlocking {
    val eventHandler = EventHandler()
    eventHandler.startProcessing()

    val layer = Layer(
        initialHalfSideLength = 5.0f,
        maxHalfSideLength = 50.0f,
        maxNeuronCount = 1000L,
        growthThreshold = 0.1f,
        eventHandler = eventHandler
    )

    val initialPos = Position(0f, 0f, 0f)
    layer.grow(initialPos);
    println("Initial Layer: $layer")


    val growthJobs = List(2000) {
        launch(Dispatchers.IO) {
            val randomX = Random.nextFloat() * layer.getCurrentHalfSideLength() * 2 - layer.getCurrentHalfSideLength()
            val randomY = Random.nextFloat() * layer.getCurrentHalfSideLength() * 2 - layer.getCurrentHalfSideLength()
            val randomZ = Random.nextFloat() * layer.getCurrentHalfSideLength() * 2 - layer.getCurrentHalfSideLength()
            val targetPos = Position(truncate(randomX), truncate(randomY), truncate(randomZ))


            val existingNeurons = layer.getAllNeurons()
            if (existingNeurons.isNotEmpty()) {
                val source = existingNeurons.random()
                layer.grow(targetPos)
            }
            delay(Random.nextLong(1, 5))
        }
    }

    growthJobs.joinAll()

    println("Final Layer: $layer")
    println("Total neurons: ${layer.getNeuronCount()}")


    eventHandler.stopProcessing()
    println("Event handler stopped.")
}

--- 合并自: Layer.kt ---

package com.treevalue.quick

import kotlinx.coroutines.*
import kotlin.random.Random
import org.nd4j.linalg.api.ndarray.INDArray
import org.nd4j.linalg.factory.Nd4j
import java.util.concurrent.ConcurrentLinkedQueue
import java.util.concurrent.atomic.AtomicReference
import java.util.concurrent.Executors
import java.util.concurrent.TimeUnit
import java.util.concurrent.atomic.AtomicBoolean
import kotlin.math.exp
import kotlin.math.sqrt


data class Position(val x: Float, val y: Float, val z: Float) {
    fun distanceTo(other: Position): Float {
        val dx = x - other.x
        val dy = y - other.y
        val dz = z - other.z

        return sqrt(dx * dx + dy * dy + dz * dz)
    }
}


const val SIGNAL_EXCITATORY_INHIBITORY_IDX = 0
const val SIGNAL_REWARD_IDX = 1
const val SIGNAL_VECTOR_SIZE = 2

const val FIRING_THRESHOLD = 0.8f
const val DEFAULT_FIRING_SIGNAL_STRENGTH = 1.0f

class Neuron(
    val coordinate: Position,
    private val eventHandler: EventHandler
) {


    private val sendTo: MutableMap<Neuron, Float> = mutableMapOf()


    private val incomingSignalBuffer = AtomicReference(ConcurrentLinkedQueue<INDArray>())


    /**
     * Adds a connection to a downstream neuron.
     * Usually called during network setup.
     */
    fun addConnection(targetNeuron: Neuron) {
        if (targetNeuron != this) {
            val distance = this.coordinate.distanceTo(targetNeuron.coordinate)
            sendTo[targetNeuron] = distance
        }
    }

    /**
     * Receives an incoming signal tensor. Thread-safe and fast.
     */
    fun receive(tensor: INDArray) {

        if (tensor.size(1) == SIGNAL_VECTOR_SIZE.toLong() && tensor.rank() == 2 && tensor.size(0) == 1L) {
            incomingSignalBuffer.get().offer(tensor.dup())
        } else {
            System.err.println("Neuron at ${this.coordinate} received malformed signal: shape ${tensor.shapeInfoToString()}")
        }
    }

    /**
     * Processes all signals received in the last cycle. Called periodically.
     */
    fun processReceivedSignals() {

        val signalsToProcess = incomingSignalBuffer.getAndSet(ConcurrentLinkedQueue())

        if (signalsToProcess.isEmpty()) {
            return
        }

        var excitatoryInhibitorySum = 0.0f
        var rewardSum = 0.0f


        while (true) {
            val signal = signalsToProcess.poll() ?: break
            try {
                excitatoryInhibitorySum += signal.getFloat(SIGNAL_EXCITATORY_INHIBITORY_IDX)
                rewardSum += signal.getFloat(SIGNAL_REWARD_IDX)
            } catch (e: Exception) {

                System.err.println("Neuron at ${this.coordinate} error processing signal content: ${signal}. Error: ${e.message}")
            }
        }


        if (excitatoryInhibitorySum > FIRING_THRESHOLD) {
            fire(excitatoryInhibitorySum, rewardSum)
        }
    }


    /**
     * Generates and sends firing events to the EventHandler for connected neurons.
     */
    private fun fire(triggerStrength: Float, accumulatedReward: Float) {

        val outgoingSignal = Nd4j.create(
            floatArrayOf(DEFAULT_FIRING_SIGNAL_STRENGTH, 0.0f),
            1L,
            SIGNAL_VECTOR_SIZE.toLong()
        )


        sendTo.forEach { (targetNeuron, distance) ->
            val event = SignalEvent(
                sourceNeuron = this,
                targetNeuron = targetNeuron,
                signal = outgoingSignal,
                distance = distance
            )
            eventHandler.submitEvent(event)
        }
    }


    override fun toString(): String {

        return "Neuron(coordinate=$coordinate, connections=${sendTo.size})"
    }
}

/**
 * Represents a signal transmission event. Uses Neuron object references.
 */
data class SignalEvent(
    val sourceNeuron: Neuron,
    val targetNeuron: Neuron,
    val signal: INDArray,
    val distance: Float
)


const val DECAY_CONSTANT = 0.1f

fun calculateDecayFactor(distance: Float): Float {
    val nonNegativeDistance = if (distance < 0f) 0f else distance
    return exp(-DECAY_CONSTANT * nonNegativeDistance)
}


class EventHandler {

    private val eventQueue = ConcurrentLinkedQueue<SignalEvent>()

    private val processingExecutor = Executors.newSingleThreadExecutor { r ->
        Thread(r, "EventHandlerThread").apply { isDaemon = true }
    }
    private val isRunning = AtomicBoolean(false)

    /**
     * Neurons call this method to submit a firing event. Thread-safe.
     */
    fun submitEvent(event: SignalEvent) {
        eventQueue.offer(event)
    }

    /**
     * Starts the event processing loop in a background thread.
     */
    fun startProcessing() {
        if (isRunning.compareAndSet(false, true)) {
            processingExecutor.submit {

                while (isRunning.get() && !Thread.currentThread().isInterrupted) {
                    processBatch()

                    if (eventQueue.isEmpty()) {
                        try {

                            Thread.sleep(0, 50_000)
                        } catch (ie: InterruptedException) {
                            Thread.currentThread().interrupt()
                        }
                    }
                }

            }
        }
    }

    /**
     * Processes a batch of events currently in the queue.
     */
    private fun processBatch() {
        var processedCount = 0
        val batchSize = 1000

        while (processedCount < batchSize) {
            val event = eventQueue.poll() ?: break

            try {
                processSingleEvent(event)
                processedCount++
            } catch (e: Exception) {
                System.err.println("EventHandler error processing event for target ${event.targetNeuron.coordinate}. Error: ${e.message}")

            }
        }
    }

    /**
     * Processes a single signal event: calculates decay, calls receive on target.
     */
    private fun processSingleEvent(event: SignalEvent) {
        val targetNeuron = event.targetNeuron
        val decayFactor = calculateDecayFactor(event.distance)
        val decayedSignalTensor = event.signal.dup()
        val originalStrength = decayedSignalTensor.getFloat(SIGNAL_EXCITATORY_INHIBITORY_IDX)
        decayedSignalTensor.putScalar(intArrayOf(0, SIGNAL_EXCITATORY_INHIBITORY_IDX), originalStrength * decayFactor)
        targetNeuron.receive(decayedSignalTensor)


    }


    /**
     * Stops the event processing loop gracefully.
     */
    fun stopProcessing() {
        isRunning.set(false)
        processingExecutor.shutdown()
        try {
            if (!processingExecutor.awaitTermination(2, TimeUnit.SECONDS)) {
                System.err.println("EventHandler shutdown timed out, forcing.")
                processingExecutor.shutdownNow()
            }
        } catch (ie: InterruptedException) {
            processingExecutor.shutdownNow()
            Thread.currentThread().interrupt()
        }
    }
}


fun main() = runBlocking {
    println("Setting up neural simulation...")

    val eventHandler = EventHandler()

    val numNeurons = 100
    val neurons = List(numNeurons) {
        Neuron(
            coordinate = Position(
                Random.nextFloat() * 100,
                Random.nextFloat() * 100,
                Random.nextFloat() * 100
            ),
            eventHandler = eventHandler
        )
    }
    println("Created ${neurons.size} neurons.")


    val connectionsPerNeuron = 5
    neurons.forEach { sourceNeuron ->
        repeat(connectionsPerNeuron) {
            var targetNeuron: Neuron
            do {

                targetNeuron = neurons.random()
            } while (targetNeuron == sourceNeuron)
            sourceNeuron.addConnection(targetNeuron)
        }
    }
    println("Created connections.")

    eventHandler.startProcessing()

    val simulationDurationMs = 100L
    val tickDurationMs = 1L

    println("Starting simulation for $simulationDurationMs ms...")
    val startTime = System.currentTimeMillis()

    val simulationJob = launch(Dispatchers.Default) {
        for (tick in 1..simulationDurationMs) {
            val tickStart = System.nanoTime()

            if (tick % 10 == 1L) {
                val targetNeuron = neurons.random()
                val externalSignal =
                    Nd4j.create(floatArrayOf(0.9f, 0.0f), 1L, SIGNAL_VECTOR_SIZE.toLong())
                targetNeuron.receive(externalSignal)
            }

            delay(0)

            neurons.forEach { neuron ->
                neuron.processReceivedSignals()
            }

            val tickEnd = System.nanoTime()
            val elapsedNs = tickEnd - tickStart
            val delayNs= (tickDurationMs * 1_000_000) - elapsedNs
            if (delayNs > 0) {
                delay(delayNs / 1_000_000)
            } else if (elapsedNs > tickDurationMs * 1_000_000 * 1.1) {
                println("Warning: Tick $tick took longer than ${tickDurationMs}ms (${elapsedNs / 1_000_000.0}ms)")
            }

            if (tick % 50 == 0L || tick == simulationDurationMs) println("Tick $tick completed.")
        }
    }


    simulationJob.join()

    val endTime = System.currentTimeMillis()
    println("Simulation finished after ${endTime - startTime} ms wall clock time.")


    println("Stopping EventHandler...")
    eventHandler.stopProcessing()
    println("Cleanup complete.")
}
